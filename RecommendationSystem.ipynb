{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ichekhovskikh/doc2vec/blob/master/RecommendationSystem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoLW8xsaGFWr",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Выполнить ипморт пакетов\n",
        "\n",
        "!pip install pymorphy2[fast]\n",
        "\n",
        "import gensim\n",
        "import os\n",
        "import collections\n",
        "import smart_open\n",
        "import random\n",
        "import json\n",
        "import urllib.request\n",
        "import pymorphy2\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem\n",
        "from IPython.display import HTML, display\n",
        "from tabulate import tabulate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzY-WMJjJhtR",
        "colab_type": "text"
      },
      "source": [
        "## Опредлим функцию для чтения и предварительной обработки текста\n",
        "Ниже мы определяем функцию для открытия  train/test файла, предварительно обрабатываем каждый текст датасета, используя простой инструмент предварительной обработки gensim (то есть, разбиваем текст на отдельные слова, удалите знаки препинания, установите строчные буквы и т. д.), лемматизацию, удаление стоп слов и возвращаем список слов. Для обучения модели нам нужно будет связать тег с каждым документом учебного корпуса. В нашем случае тег - это идентификатор статьи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFO_znw317kM",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Определить функцию лемматизации\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def lemmatize(words):\n",
        "    for word in words:\n",
        "        yield morph.parse(word)[0].normal_form"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRm8tmSe17Hq",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Определить функцию удаление стоп слов\n",
        "\n",
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    return [word for word in words if word not in russian_stopwords]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHwqCiuN-hDS",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Определить функцию предобработки текста\n",
        "\n",
        "def advanced_preprocess(text):\n",
        "    normalized_text = gensim.utils.simple_preprocess(text)\n",
        "    normalized_text = list(lemmatize(normalized_text))\n",
        "    normalized_text = remove_stopwords(normalized_text)\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMO4zHrqGTws",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Определить функцию загрузки корпуса текстовых документов\n",
        "\n",
        "def read_corpus(corpus_path, tokens_only=False):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        for article in corpus:\n",
        "            normalized_text = advanced_preprocess(article['text'])\n",
        "            if tokens_only:\n",
        "                yield normalized_text\n",
        "            else:\n",
        "                yield gensim.models.doc2vec.TaggedDocument(normalized_text, article['tags'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1gSBZ1G1BNj",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Определить функцию получения исходного текста статьи по индексу\n",
        "\n",
        "def get_article_text_by_index(index, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        return corpus[index]['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-nMf23CJ_DM",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Определить функцию получения исходного текста статьи по идентификатору\n",
        "\n",
        "def get_article_text_by_id(id, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        for index in range(len(corpus)):\n",
        "          if (corpus[index]['id'] == id):\n",
        "            return corpus[index]['text']\n",
        "        return \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhMV7VB-Rn-Y",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Определить функцию получения категории статьи по индексу\n",
        "\n",
        "def get_article_class_by_index(index, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        return corpus[index]['class_name']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "opeO4COvKJDn",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Определить функцию получения категории статьи по идентификатору\n",
        "\n",
        "def get_article_class_by_id(id, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        for index in range(len(corpus)):\n",
        "          if (corpus[index]['id'] == id):\n",
        "            return corpus[index]['class_name']\n",
        "        return \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZYD5AXcNYRU",
        "colab_type": "text"
      },
      "source": [
        "#Начинаем\n",
        "Для начала нам понадобится комплект документов для обучения нашей модели doc2vec. Теоретически, документ может быть чем угодно: коротким твитом из 140 символов, отдельным абзацем, новостной статьей или книгой. В NLP комплект документов часто называют корпусом.\n",
        "\n",
        "Будем тренировать нашу модель на собственном корпусе. Этот корпус содержит 500 научных статей на 5 различных тем.\n",
        "\n",
        "Для тестирования будет использоваться тестовый корпус из 50 статей (10 статей на каждую тему).\n",
        "\n",
        "Dataset состоит из трех строк: id (идентификатор строки), text (текст статьи), tag (идентификатор самой статьи, вектор которого будем обучать), class_name (класс текста, был размечен вручную, необходим только для тестирования)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0vJrTipGemC",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Введите путь к файлам исходной базы статей:\n",
        "\n",
        "train_path = 'https://raw.githubusercontent.com/ichekhovskikh/cyberleninka-article-downloader/master/train_corpus.json' #@param {type: \"string\"}\n",
        "test_path = 'https://raw.githubusercontent.com/ichekhovskikh/cyberleninka-article-downloader/master/test_corpus.json' #@param {type: \"string\"}\n",
        "\n",
        "train_corpus = []\n",
        "test_corpus = []\n",
        "\n",
        "try:\n",
        "    train_corpus = list(read_corpus(train_path))\n",
        "    test_corpus = list(read_corpus(test_path, tokens_only=True))\n",
        "    print(\"Загружено.\")\n",
        "except:\n",
        "    print(\"Невозможно открыть файл! Проверьте формат выбранных файлов.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9aPeDPvGgve",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Давайте посмотрим на учебный корпус:\n",
        "\n",
        "print(train_corpus[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrWLt9Z8Gm66",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Корпус тестирования выглядит так:\n",
        "\n",
        "print(test_corpus[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DB1AKHoKvt3",
        "colab_type": "text"
      },
      "source": [
        "Обратите внимание, что корпус тестирования представляет собой просто список списков и не содержит никаких тегов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luOlm4Q5K2jj",
        "colab_type": "text"
      },
      "source": [
        "# Обучение модели\n",
        "## Создание объекта Doc2Vec\n",
        "Теперь мы создадим модель Doc2Vec с векторным размером 80 слов и перебираем учебный корпус 200 раз (данные параметры были получены после проведения ряда исследований).\n",
        "\n",
        "Словарь содержит в себе все уникальные слова, извлеченных из учебного корпуса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6w0x7RlnzVD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Укажите параметры обучения модели:\n",
        "\n",
        "vector_size = 80 #@param\n",
        "window =  3 #@param\n",
        "epochs =  200 #@param\n",
        "alpha = 0.001 #@param\n",
        "learning_method = 'PV-DM' #@param [\"PV-DM\", \"PV-DBOW\"] {type:\"string\"}\n",
        "\n",
        "dm = 1\n",
        "if (learning_method == 'PV-DBOW'):\n",
        "    dm = 0\n",
        "\n",
        "model = gensim.models.doc2vec.Doc2Vec(\n",
        "    vector_size=vector_size, \n",
        "    dm=dm,\n",
        "    window=window,\n",
        "    alpha=alpha, \n",
        "    epochs=epochs)\n",
        "\n",
        "model.build_vocab(train_corpus)\n",
        "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfha632uMsgP",
        "colab_type": "text"
      },
      "source": [
        "## Inferring a Vector\n",
        "Важно отметить, что теперь вы можете вывести вектор для любого фрагмента текста без необходимости переобучать модель, передав список слов в функцию model.infer_vector. Затем этот вектор можно сравнить с другими векторами по косинусной близости."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwpCdZr0HJ0O",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Получить вектор текста:\n",
        "\n",
        "text_sample = \"Горностай небольшой зверек семейства куньих очень ценится\" #@param {type:\"string\"}\n",
        "\n",
        "model.infer_vector(advanced_preprocess(text_sample))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZSVoxQNM9ji",
        "colab_type": "text"
      },
      "source": [
        "Обратите внимание, что, поскольку лежащие в основе алгоритмы обучения представляют собой подходы на основе итеративной аппроксимации, в которой используется внутренняя рандомизация, таким обрахрм, повторные выводы одного и того же текста будут возвращать слегка разные векторы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRfMjEATNm-y",
        "colab_type": "text"
      },
      "source": [
        "# Оценочная модель\n",
        "Чтобы оценить нашу новую модель, мы сначала выведем новые векторы для каждого документа тренировочного корпуса, сравним выведенные векторы с тренировочным корпусом.\n",
        "\n",
        "Проверка выведенного вектора по обучающему вектору является своего рода «проверкой работоспособности» в отношении того, ведет ли модель себя адекватно, хотя и не является реальным значением «точности».\n",
        "\n",
        "Можем взглянуть на пример:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTpgGnFHgH0",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Проверить модель на адекватьность на глаз\n",
        "\n",
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "\n",
        "inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "sims = model.docvecs.most_similar([inferred_vector], topn=5)\n",
        "tabledata = [[\"Индекс\", \"Схожесть\", \"Категория\", \"Текст\"]]\n",
        "\n",
        "print('\\nТЕКСТ ИСХДНОГО ДОКУМЕНТА ({}) «{}»: «{}»\\n'.format(doc_id, get_article_class_by_id(doc_id, train_path), get_article_text_by_index(doc_id, train_path)))\n",
        "for label, index in [('1)', 0), ('2)', 1), ('3)', 2), ('4)', 3), ('5)', 4)]:\n",
        "    tabledata.append([label, sims[index], get_article_class_by_id(int(sims[index][0]), train_path), get_article_text_by_id(int(sims[index][0]), train_path)])\n",
        "\n",
        "print(tabulate(tabledata))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdbwS5KoOeTo",
        "colab_type": "text"
      },
      "source": [
        "Обратите внимание, что наиболее похожий документ (как правило, тот же текст) имеет параметр сходства, приближающийся к единице. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eq8rXTHOwBQ",
        "colab_type": "text"
      },
      "source": [
        "# Тестирование модели\n",
        "Используя тот же подход, что и выше, мы выведем вектор для случайно выбранного тестового документа и сравним документ с нашей моделью с помощью f1-меры."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5yDhL0JHy9v",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Выполнить тестирование обученной модели\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "test_size = len(test_corpus)\n",
        "\n",
        "predicted_classes = [] \n",
        "test_classes = []\n",
        "\n",
        "for doc_index in range(test_size):\n",
        "    inferred_vector = model.infer_vector(test_corpus[doc_index])\n",
        "    sims = model.docvecs.most_similar([inferred_vector], topn=5)\n",
        "\n",
        "    test_article_class = get_article_class_by_index(doc_index, test_path)\n",
        "    tabledata = [[\"Индекс\", \"Схожесть\", \"Категория\", \"Текст\"]]\n",
        "    print('\\nТЕКСТ ИСХДНОГО ДОКУМЕНТА ({}) «{}»: «{}»\\n'.format(doc_index, test_article_class, get_article_text_by_index(doc_index, test_path)))\n",
        "    for label, index in [('1)', 0), ('2)', 1), ('3)', 2), ('4)', 3), ('5)', 4)]:\n",
        "        predicted_article_class = get_article_class_by_id(int(sims[index][0]), train_path)\n",
        "        tabledata.append([label, sims[index], predicted_article_class, get_article_text_by_id(int(sims[index][0]), train_path)])\n",
        "        predicted_classes.append(predicted_article_class)\n",
        "        test_classes.append(test_article_class)\n",
        "    print(tabulate(tabledata))\n",
        "\n",
        "print('f1score: {}'.format(f1_score(test_classes, predicted_classes, average='macro')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSpkRxWUrC9B"
      },
      "source": [
        "# Поиск похожих научных документов\n",
        "Выполните поиск похожих научных статей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrflInTzrgmW",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Укажите путь к тексту статьи в формате *.txt или введите текст статьи:\n",
        "article_text = '' #@param {type: \"string\"}\n",
        "article_path = '' #@param {type: \"string\"}\n",
        "\n",
        "try:\n",
        "    if (article_text == ''):\n",
        "        with urllib.request.urlopen(article_path) as article_url:\n",
        "            article_text = article_url.read().decode()\n",
        "\n",
        "    normalized_text = advanced_preprocess(article_text)\n",
        "    inferred_vector = model.infer_vector(normalized_text)\n",
        "    sims = model.docvecs.most_similar([inferred_vector], topn=5)\n",
        "\n",
        "    tabledata = [[\"Индекс\", \"Схожесть\", \"Текст\"]]\n",
        "    print('\\nТЕКСТ ИСХДНОГО ДОКУМЕНТА: «{}»\\n'.format(article_text))\n",
        "    for label, index in [('1)', 0), ('2)', 1), ('3)', 2), ('4)', 3), ('5)', 4)]:\n",
        "        tabledata.append([label, sims[index], get_article_text_by_id(int(sims[index][0]), train_path)])\n",
        "    print(tabulate(tabledata))\n",
        "except:\n",
        "  print(\"Ошибка поиска! Проверьте правильность ввода.\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}